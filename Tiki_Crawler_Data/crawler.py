import requests
import json
import time
import os
import config


def crawl_products(page=1):
    """C√†o d·ªØ li·ªáu s·∫£n ph·∫©m c·ªßa 1 trang"""
    url = (
        f"https://tiki.vn/api/personalish/v1/blocks/listings"
        f"?limit={config.LIMIT}&sort=top_seller"
        f"&page={page}&urlKey={config.URL_KEY}&category={config.CATEGORY_ID}"
    )
    headers = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64)"}

    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        data = res.json().get("data", [])
        return data
    except Exception as e:
        print(f"‚ö†Ô∏è Trang {page} l·ªói: {e}")
        return []


def extract_fields(product):
    """L·∫•y c√°c tr∆∞·ªùng c·∫ßn thi·∫øt"""
    amp = product.get("visible_impression_info", {}).get("amplitude", {})
    return {
        "id": product.get("id"),
        "title": product.get("name"),
        "price": product.get("price"),
        "discount": product.get("discount_rate"),
        "rating": float(product.get("rating_average") or 0),
        "reviews": product.get("review_count"),
        "quantity_sold": (product.get("quantity_sold") or {}).get("value", 0),
        "original_price": product.get("original_price", 0),
        "category_l1": amp.get("category_l1_name", ""),
        "category_l2": amp.get("category_l2_name", ""),
        "category_l3": amp.get("category_l3_name", ""),
        "category_l4": amp.get("category_l4_name", ""),
        "primary_category": amp.get("primary_category_name", ""),
        "url": f"https://tiki.vn/{product.get('url_path', '')}"
    }


def save_crawled_data():
    """C√†o to√†n b·ªô danh m·ª•c v√† l∆∞u JSON"""
    os.makedirs(config.DATA_DIR, exist_ok=True)
    total = 0
    all_data = []

    print(f"üöÄ B·∫Øt ƒë·∫ßu crawl danh m·ª•c: {config.URL_KEY} (ID={config.CATEGORY_ID})")

    for page in range(1, config.MAX_PAGE + 1):
        print(f"\nüì¶ ƒêang crawl trang {page} ...")
        products = crawl_products(page)
        if not products:
            print("‚úÖ H·∫øt d·ªØ li·ªáu ho·∫∑c b·ªã ch·∫∑n, d·ª´ng crawl.")
            break

        filtered = [extract_fields(p) for p in products]

        # L∆∞u t·ª´ng trang ri√™ng
        page_file = os.path.join(config.DATA_DIR, f"tiki_page_{page}.json")
        with open(page_file, "w", encoding="utf-8") as f:
            json.dump(filtered, f, ensure_ascii=False, indent=2)
        print(f"üíæ ƒê√£ l∆∞u {len(filtered)} s·∫£n ph·∫©m ‚Üí {page_file}")

        all_data.extend(filtered)
        total += len(filtered)
        time.sleep(1.5)

    # G·ªôp to√†n b·ªô v√†o file t·ªïng
    all_file = os.path.join(config.DATA_DIR, "tiki_all_books.json")
    with open(all_file, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"\nüéâ Crawl ho√†n t·∫•t ({total} s·∫£n ph·∫©m). File t·ªïng: {all_file}")


if __name__ == "__main__":
    save_crawled_data()
