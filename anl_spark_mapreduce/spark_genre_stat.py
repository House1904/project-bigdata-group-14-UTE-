from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    explode, split, trim, col, lower, count, avg,
    rank, round, desc
)
from pyspark.sql.window import Window

# ============================================================
# 1Ô∏è‚É£ Kh·ªüi t·∫°o SparkSession
# ============================================================
spark = SparkSession.builder.appName("GenreAdvancedStats").getOrCreate()

# ============================================================
# 2Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS (chu·∫©n ƒë·ªãnh d·∫°ng tab v√† quote)
# ============================================================
input_path = "hdfs:///user/hive/warehouse/anl_anime/part-m-00000"

df = (
    spark.read
    .option("header", "false")
    .option("inferSchema", "true")
    .option("sep", "\t")          
    .option("quote", '"')         
    .csv(input_path)
)

# ============================================================
# 3Ô∏è‚É£ G√°n t√™n c·ªôt (theo c·∫•u tr√∫c AniList)
# ============================================================
df = df.toDF(
    "id", "title_romaji", "title_english", "title_native", "format",
    "episodes", "duration", "source", "country", "isAdult",
    "score", "meanScore", "popularity", "favourites", "trending",
    "genres", "tags", "status", "studio", "season",
    "start_year", "url"
)

# ============================================================
# 4Ô∏è‚É£ L√†m s·∫°ch d·ªØ li·ªáu c∆° b·∫£n
# ============================================================
df_clean = df.filter(
    (col("genres").isNotNull()) &
    (col("score").isNotNull()) &
    (col("popularity").isNotNull()) &
    (col("studio").isNotNull())
)

# ============================================================
# 5Ô∏è‚É£ X·ª≠ l√Ω c·ªôt "genres": t√°ch & chu·∫©n h√≥a t·ª´ng th·ªÉ lo·∫°i
# ============================================================
genres_df = (
    df_clean
    .withColumn("genre", explode(split(col("genres"), ",")))
    .select(
        trim(lower(col("genre"))).alias("genre"),
        col("score").cast("double").alias("score"),
        col("popularity").cast("int").alias("popularity"),
        trim(col("studio")).alias("studio"),
    )
    .filter(col("genre") != "")
)

# ============================================================
# 6Ô∏è‚É£ Th·ªëng k√™ c∆° b·∫£n theo genre
# ============================================================
genre_stats = (
    genres_df.groupBy("genre")
    .agg(
        count("*").alias("anime_count"),
        round(avg("score"), 2).alias("avg_score"),
        round(avg("popularity"), 0).alias("avg_popularity")
    )
)

# ============================================================
# 7Ô∏è‚É£ T√≠nh studio ph·ªï bi·∫øn nh·∫•t trong t·ª´ng genre
# ============================================================
studio_count = genres_df.groupBy("genre", "studio").agg(
    count("*").alias("studio_count")
)

windowSpec = Window.partitionBy("genre").orderBy(desc("studio_count"))

top_studios = (
    studio_count
    .withColumn("rank", rank().over(windowSpec))
    .filter(col("rank") == 1)
    .select("genre", col("studio").alias("top_studio"))
)

# ============================================================
# 8Ô∏è‚É£ G·ªôp k·∫øt qu·∫£ th·ªëng k√™ & studio top
# ============================================================
final_result = (
    genre_stats.join(top_studios, on="genre", how="left")
    .orderBy(col("anime_count").desc())
)

# ============================================================
# 9Ô∏è‚É£ Ghi k·∫øt qu·∫£ ra HDFS
# ============================================================
output_path = "hdfs:///user/hive/warehouse/output/genre_advanced_stats"

final_result.write.mode("overwrite").option("header", "true").csv(output_path)

# ============================================================
# üîü Hi·ªÉn th·ªã top 10 ra console
# ============================================================
print("=== üé¨ Genre Advanced Stats (Top 10) ===")
final_result.show(10, truncate=False)

# ============================================================
# üîö D·ª´ng SparkSession
# ============================================================
spark.stop()
